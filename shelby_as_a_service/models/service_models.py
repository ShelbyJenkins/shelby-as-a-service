import os
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Optional
from dotenv import load_dotenv
from services.deployment_service.deployment_management import DeploymentManager
from services.log_service import Logger

@dataclass
class IndexModel:
    
    index_name: Optional[str] = None
    index_env: str = 'us-central1-gcp'
    index_embedding_model: str = 'text-embedding-ada-002'
    index_tiktoken_encoding_model: str = 'text-embedding-ada-002'
    index_embedding_max_chunk_size: int = 8191
    index_embedding_batch_size: int = 100
    index_vectorstore_dimension: int = 1536
    index_vectorstore_upsert_batch_size: int = 20
    index_vectorstore_metric: str = 'cosine'
    index_vectorstore_pod_type: str = 'p1'
    index_preprocessor_min_length: int = 150
    # index_text_splitter_goal_length: int = 500
    index_text_splitter_goal_length: int = 750
    index_text_splitter_overlap_percent: int = 15  # In percent
    index_openai_timeout_seconds: float = 180.0
    index_indexed_metadata = [
        'data_domain_name',
        'data_source_name',
        'doc_type',
        'target_type',
        'date_indexed',
    ]

    service_name_: str = 'index_service'
    required_variables_: List[str] = field(default_factory=lambda: ['index_name', 'index_env'])
    required_secrets_: List[str] = field(default_factory=lambda: ['openai_api_key', 'pinecone_api_key'])
    
    
@dataclass
class CEQModel:

    # ActionAgent
    action_llm_model: str = 'gpt-4'
    # QueryAgent
    ceq_index_name: Optional[str] = None
    ceq_index_env: Optional[str] = None
    ceq_data_domain_constraints_enabled: bool = False
    ceq_data_domain_constraints_llm_model: str = 'gpt-4'
    ceq_data_domain_none_found_message: str = 'Query not related to any supported data domains (aka topics). Supported data domains are:'
    ceq_keyword_generator_enabled: bool = False
    ceq_keyword_generator_llm_model: str = 'gpt-4'
    ceq_doc_relevancy_check_enabled: bool = False
    ceq_doc_relevancy_check_llm_model: str = 'gpt-4'
    ceq_embedding_model: str = 'text-embedding-ada-002'
    ceq_tiktoken_encoding_model: str = 'text-embedding-ada-002'
    ceq_docs_to_retrieve: int = 5
    ceq_docs_max_token_length: int = 1200
    ceq_docs_max_total_tokens: int = 3500
    ceq_docs_max_used: int = 5
    ceq_main_prompt_llm_model: str = 'gpt-4'
    ceq_max_response_tokens: int = 300
    openai_timeout_seconds: float = 180.0
    # APIAgent
    api_agent_select_operation_id_llm_model: str = 'gpt-4'
    api_agent_create_function_llm_model: str = 'gpt-4'
    api_agent_populate_function_llm_model: str = 'gpt-4'
    
    service_name_: str = 'ceq_agent'
    required_variables_: List[str] = field(default_factory=lambda: ['ceq_index_name', 'ceq_index_env'])
    required_secrets_: List[str] = field(default_factory=lambda: ['openai_api_key', 'pinecone_api_key'])


@dataclass
class DiscordModel:

    discord_enabled_servers: List[str] = field(default_factory=list) 
    discord_specific_channels_enabled: bool = False
    discord_specific_channel_ids: List[str] = field(default_factory=list) 
    discord_all_channels_enabled: bool = False
    discord_all_channels_excluded_channels: List[str] = field(default_factory=list) 
    discord_manual_requests_enabled: bool = True
    discord_auto_response_enabled: bool = False
    discord_auto_response_cooldown: int = 10
    discord_auto_respond_in_threads: bool = False
    discord_user_daily_token_limit: int = 30000
    discord_welcome_message: str = 'ima tell you about the {}.'
    discord_short_message: str = '<@{}>, brevity is the soul of wit, but not of good queries. Please provide more details in your request.'
    discord_message_start: str = 'Running request... relax, chill, and vibe a minute.'
    discord_message_end: str = 'Generated by: gpt-4. Memory not enabled. Has no knowledge of past or current queries. For code see https://github.com/shelby-as-a-service/shelby-as-a-service.'

    service_name_: str = 'discord_sprite'
    required_variables_: List[str] = field(default_factory=lambda: ['discord_enabled_servers'])
    required_secrets_: List[str] = field(default_factory=lambda: ['discord_bot_token'])


@dataclass
class SlackModel:

    slack_enabled_teams: List[str] = field(default_factory=list) 
    slack_welcome_message: str = 'ima tell you about the {}.'
    slack_short_message: str = '<@{}>, brevity is the soul of wit, but not of good queries. Please provide more details in your request.'
    slack_message_start: str = 'Relax and vibe while your query is embedded, documents are fetched, and the LLM is prompted.'
    slack_message_end: str = 'Generated by: gpt-4. Memory not enabled. Has no knowledge of past or current queries. For code see https://github.com/shelby-as-a-service/shelby-as-a-service.'

    service_name_: str = 'slack_sprite'
    required_variables_: List[str] = field(default_factory=lambda: ['slack_enabled_teams'])
    required_secrets_: List[str] = field(default_factory=lambda: ['slack_app_token', 'slack_bot_token'])


@dataclass
class LocalModel:

    default_deployment_enabled: bool = True
    default_local_deployment_name: Optional[str] = None
    local_message_start: str = 'Running request... relax, chill, and vibe a minute.'
    local_message_end: str = 'Generated by: gpt-4. Memory not enabled. Has no knowledge of past or current queries. For code see https://github.com/shelby-as-a-service/shelby-as-a-service.'

    service_name_: str = 'local_sprite'
    required_variables_: List[str] = field(default_factory=list) 
    required_secrets_: List[str] = field(default_factory=list) 
    
    
# @dataclass
# class ContainerDeploymentModel:
    
#     required_variables_ = ['docker_registry', 'docker_username', 'docker_repo']
#     required_secrets_ = [
#         'docker_token',
#         'stackpath_stack_slug',
#         'stackpath_client_id',
#         'stackpath_api_client_secret',
#     ]

#     docker_registry: Optional[str] = None
#     docker_username: Optional[str] = None
#     docker_repo: Optional[str] = None

@dataclass
class DeploymentModel:
    
    service_name_: str = 'deployment_instance'
    enabled_sprites: List[str] = field(default_factory=lambda: ['local_sprite'])
    required_variables_: List[str] = field(default_factory=lambda: ['enabled_sprites'])
    
class ServiceBase:
    """Base model for all services.
    Child classes have access to all class variables of ServiceBase with self.variable.
    setup_config uses asdict to load settings from models, configs, and function params.
    It then loads child services by passing a config file and instantiating the service.
    """
    secrets: Dict[str, str] = {}
    deployment_name: str = 'base'
    
    def setup_config(self, service_config = None, **kwargs):
        
        # Initial call
        if service_config is None:
            config_from_file = DeploymentManager.load_deployment_file(
                self.deployment_name, self.model_.service_name_
            )
            load_dotenv(os.path.join(f"shelby_as_a_service/deployments/{self.deployment_name}/", ".env"))
        else:
            config_from_file = service_config[self.model_.service_name_]
            
        if config_from_file is None:
            print(f'R N R! config_from_file for {self.model_.service_name_} not found!')
            return None
        
        if hasattr(self, 'required_sprites_'):
            for sprite in self.required_sprites_:
                sprite_instance = sprite(self)
                setattr(self, sprite.model_.service_name_, sprite_instance)
        
        if hasattr(self, 'required_services_') and self.required_services_ is not None:
            for service in self.required_services_:
                service_instance = service(config_from_file['services'])
                setattr(self, service.model_.service_name_, service_instance)
                
        if hasattr(self.model_, 'required_secrets_'):
            ServiceBase.load_secrets(self.model_)
            
        merged_config = {**asdict(self.model_), **config_from_file, **kwargs}
            
        if merged_config.get('services', None):
            merged_config.pop('services')
        
        for key, value in merged_config.items():
            setattr(self, key, value)
    
    @classmethod
    def load_secrets(cls, model):
        for secret in model.required_secrets_:
            secret_str = f"{cls.deployment_name}_{secret}"
            secret_str = secret_str.upper()
            env_secret = os.environ.get(secret_str, None)
            if env_secret in [None, '']:
                print(f"Secret: {secret_str} is None!")
            cls.secrets[secret] = 'test'
        
        
    # def check_secrets(self, model_secrets):
    #     """For disabling services lacking secrets"""
    #     for secret in model_secrets:
    #         if not self.secrets.get(secret):
    #             return False

    #     return True

