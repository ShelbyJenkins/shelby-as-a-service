# region
import os
import asyncio
from concurrent.futures import ThreadPoolExecutor
import shutil
from typing import Dict, Optional, List, Any
from pydantic import BaseModel
import gradio as gr
from agents.ceq_agent import CEQAgent
from agents.vanilla_agent import VanillaChatAgent
from agents.web_agent import WebAgent

from sprites.web.gradio_interface import GradioInterface
import sprites.web.interface_helpers as help
import modules.utils.config_manager as ConfigManager

# endregion


class WebSprite:
    sprite_name: str = "web_sprite"
    app: Optional[Any] = None
    index: Optional[Any] = None
    ui: Dict[Any, Any] = {}

    # default_deployment_enabled: bool = True
    # default_local_deployment_name: Optional[str] = None
    local_message_start: str = "Running request... relax, chill, and vibe a minute."
    local_message_end: str = "Generated by: gpt-4. Memory not enabled. Has no knowledge of past or current queries. For code see https://github.com/shelby-as-a-service/shelby-as-a-service."

    def __init__(self, app):
        """ """
        self.app = app
        self.index = self.app.index
        self.log = self.app.log
        ConfigManager.setup_service_config(self)

        # self.ceq_agent = CEQAgent(self)
        self.vanillm_agent = VanillaChatAgent(self)
        self.web_agent = WebAgent(self)

        self.gradio_interface = GradioInterface(self)

    def run_chat(self, *state_vals):
        state_dict = help.comp_values_to_dict(self.ui, *state_vals)

        if agent := getattr(self, state_dict["group_name"], None):
            yield from agent.create_streaming_chat(
                state_dict["chat_tab_in_text"],
                stream=True,
                provider_name=state_dict["chat_llm_provider"],
                model_name=state_dict["chat_llm_model"],
            )

    def _log(self, message):
        self.log.print_and_log_gradio(message)
        gr.Info(message)

    def run_sprite(self):
        self.gradio_interface.create_interface()
